---
version: '2'
services:
  connect-replicator:    
    #image: ${REPOSITORY}/cp-kafka-connect:${CONFLUENT_DOCKER_TAG}
    image: ${REPOSITORY}/cp-enterprise-replicator:${CONFLUENT_DOCKER_TAG}    
    hostname: connect-replicator
    container_name: connect-replicator
    ports:
      - "8083:8083"
      - "9892:9892"
    command:
      - bash
      - -c
      - |
        confluent-hub install --no-prompt confluentinc/kafka-connect-datagen:0.6.0
        cp /usr/share/java/kafka-connect-replicator/replicator-rest-extension-*.jar /etc/kafka-connect/jars/
        /etc/confluent/docker/run
    environment:
      CONNECT_BOOTSTRAP_SERVERS: ${TARGET_CLUSTER_BOOTSTRAP}
      CONNECT_GROUP_ID: "connect_replicator_cg"
      CONNECT_REST_ADVERTISED_HOST_NAME: connect-replicator      
      CONNECT_LISTENERS: http://0.0.0.0:8083      
      CONNECT_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONNECT_SECURITY_PROTOCOL: SASL_SSL
      CONNECT_SASL_MECHANISM: PLAIN      
      CONNECT_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${TARGET_CLUSTER_OWNER_API_KEY}" password="${TARGET_CLUSTER_OWNER_API_SECRET}";
      
      ## THESE CONVERTERS SHOULD BE OVERRIDEN BY EACH CONNECTOR AS IT SPECIFIES HOW THE DATA IS STORED-IN/LOADED-FROM KAFKA
      #CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: false
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: false
      ## THESE CONVERTERS ARE USED FOR OFFSET AND CONFIG DATA
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_KEY_CONVERTER_SCHEMAS_ENABLE: false
      CONNECT_INTERNAL_VALUE_CONVERTER_SCHEMAS_ENABLE: false

      CONNECT_REPLICATION_FACTOR: 3
      CONNECT_CONFIG_STORAGE_TOPIC: connect-replicator-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_CONFIG_STORAGE_PARTITIONS: 1      
      CONNECT_OFFSET_STORAGE_TOPIC: connect-replicator-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_OFFSET_STORAGE_PARTITIONS: 5
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000      
      CONNECT_STATUS_STORAGE_TOPIC: connect-replicator-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_STATUS_STORAGE_PARTITIONS: 5
      
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_LOG4J_ROOT_LOGLEVEL: INFO
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR

      CONNECT_CONSUMER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL
      CONNECT_CONSUMER_SASL_MECHANISM: PLAIN
      CONNECT_CONSUMER_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${TARGET_CLUSTER_OWNER_API_KEY}" password="${TARGET_CLUSTER_OWNER_API_SECRET}";
      CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
      CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SECURITY_PROTOCOL: SASL_SSL
      CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_MECHANISM: PLAIN
      CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${TARGET_CLUSTER_OWNER_API_KEY}" password="${TARGET_CLUSTER_OWNER_API_SECRET}";

      CONNECT_PRODUCER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL
      CONNECT_PRODUCER_SASL_MECHANISM: PLAIN
      CONNECT_PRODUCER_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${TARGET_CLUSTER_OWNER_API_KEY}" password="${TARGET_CLUSTER_OWNER_API_SECRET}";
      CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
      CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SECURITY_PROTOCOL: SASL_SSL
      CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_MECHANISM: PLAIN
      CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${TARGET_CLUSTER_OWNER_API_KEY}" password="${TARGET_CLUSTER_OWNER_API_SECRET}";

      ## THIS DISABLES THE REPORTER FOR SINK EVENTS (NOT NEEDED)
      CONNECT_REPORTER_ERROR_TOPIC_NAME:
      CONNECT_REPORTER_RESULT_TOPIC_NAME: 

      # Confluent Monitoring Interceptors for Control Center Streams Monitoring
      # CONNECT_REST_EXTENSION_CLASSES: io.confluent.connect.replicator.monitoring.ReplicatorMonitoringExtension
      KAFKA_JMX_PORT: 9892
      KAFKA_JMX_HOSTNAME: localhost

      ##FOR LICENSED CONNECTORS
      # "confluent.topic.bootstrap.servers":"<cloud-bootstrap-servers>",
      # "confluent.topic.sasl.jaas.config":"org.apache.kafka.common.security.plain.PlainLoginModule required username=\"<CLUSTER_API_KEY>\" password=\"<CLUSTER_API_KEY>\";",
      # "confluent.topic.security.protocol":"SASL_SSL",
      # "confluent.topic.sasl.mechanism":"PLAIN"

  control-center:
    image: ${REPOSITORY}/cp-enterprise-control-center:${CONFLUENT_DOCKER_TAG}
    hostname: control-center
    container_name: control-center
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_MODE_ENABLE: management
      CONTROL_CENTER_BOOTSTRAP_SERVERS: ${TARGET_CLUSTER_BOOTSTRAP}
      CONTROL_CENTER_STREAMS_SECURITY_PROTOCOL: SASL_SSL
      CONTROL_CENTER_STREAMS_SASL_MECHANISM: PLAIN
      CONTROL_CENTER_STREAMS_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${TARGET_CLUSTER_OWNER_API_KEY}" password="${TARGET_CLUSTER_OWNER_API_SECRET}";
      # Workaround for MMA-3564
      CONTROL_CENTER_CONFLUENT_METRICS_TOPIC_MAX_MESSAGE_BYTES: 8388608
      # Confluent Schema Registry configuration for Confluent Control Center
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: ${TARGET_SCHEMA_REGISTRY_URL}
      CONTROL_CENTER_SCHEMA_REGISTRY_BASIC_AUTH_CREDENTIALS_SOURCE: USER_INFO
      CONTROL_CENTER_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO: ${TARGET_SCHEMA_REGISTRY_API_KEY}${TARGET_SCHEMA_REGISTRY_API_SECRET}

      # MONITORED CONFLUENT CLOUD CLUSTERS (ONLY ONE WOULD BE ENOUGH AS IT WILL BE REPLACED BY THE C3 KAFKA BACKEND)
      CONTROL_CENTER_KAFKA_SOURCE_BOOTSTRAP_SERVERS: ${SOURCE_CLUSTER_BOOTSTRAP}
      CONTROL_CENTER_KAFKA_SOURCE_SECURITY_PROTOCOL: SASL_SSL
      CONTROL_CENTER_KAFKA_SOURCE_SASL_MECHANISM: PLAIN
      CONTROL_CENTER_KAFKA_SOURCE_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${SOURCE_CLUSTER_OWNER_API_KEY}" password="${SOURCE_CLUSTER_OWNER_API_SECRET}";
      CONTROL_CENTER_KAFKA_TARGET_BOOTSTRAP_SERVERS: ${TARGET_CLUSTER_BOOTSTRAP}
      CONTROL_CENTER_KAFKA_TARGET_SECURITY_PROTOCOL: SASL_SSL
      CONTROL_CENTER_KAFKA_TARGET_SASL_MECHANISM: PLAIN
      CONTROL_CENTER_KAFKA_TARGET_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${TARGET_CLUSTER_OWNER_API_KEY}" password="${TARGET_CLUSTER_OWNER_API_SECRET}";

      # CONNECT / REPLICATOR CLUSTER
      CONTROL_CENTER_CONNECT_CONNECT-REPLICATOR_CLUSTER: http://connect-replicator:8083
      
      # WHICH OF THESE ARE REALLY NEEDED
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONTROL_CENTER_REPLICATION_FACTOR: 3
      CONFLUENT_METRICS_TOPIC_REPLICATION: 3
      
      PORT: 9021

  

  # connect-dc1:
  #   image: ${REPOSITORY}/cp-enterprise-replicator:${CONFLUENT_DOCKER_TAG}
  #   hostname: connect-dc1
  #   container_name: connect-dc1
  #   depends_on:
  #     - broker-dc1
  #     - schema-registry-dc1
  #     - broker-dc2
  #     - schema-registry-dc2
  #   ports:
  #     - "8381:8381"
  #     - "9891:9891"
  #   command: "bash -c 'cp /usr/share/java/kafka-connect-replicator/replicator-rest-extension-*.jar /etc/kafka-connect/jars/; /etc/confluent/docker/run'"
  #   environment:
  #     CONNECT_BOOTSTRAP_SERVERS: 'broker-dc1:29091'
  #     CONNECT_REST_ADVERTISED_HOST_NAME: connect-dc1
  #     CONNECT_LISTENERS: http://connect-dc1:8381
  #     CONNECT_GROUP_ID: "connect-dc1"
  #     CONNECT_PRODUCER_CLIENT_ID: "connect-worker-producer-dc1"
  #     CONNECT_CONFIG_STORAGE_TOPIC: connect-configs-dc1
  #     CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
  #     CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets-dc1
  #     CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_STATUS_STORAGE_TOPIC: connect-status-dc1
  #     CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
  #     CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
  #     CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
  #     CONNECT_PLUGIN_PATH: /usr/share/java
  #     CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
  #     # Confluent Monitoring Interceptors for Control Center Streams Monitoring
  #     CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
  #     CONNECT_PRODUCER_CONFLUENT_MONITORING_INTERCEPTOR_BOOTSTRAP_SERVERS: broker-dc2:29092
  #     CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
  #     CONNECT_CONSUMER_CONFLUENT_MONITORING_INTERCEPTOR_BOOTSTRAP_SERVERS: broker-dc2:29092
  #     CONNECT_REST_EXTENSION_CLASSES: io.confluent.connect.replicator.monitoring.ReplicatorMonitoringExtension
  #     KAFKA_JMX_PORT: 9891
  #     KAFKA_JMX_HOSTNAME: localhost

  # datagen-dc1-topic1:
  #   image: ${REPOSITORY}/ksqldb-examples:${CONFLUENT_DOCKER_TAG}
  #   hostname: datagen-dc1-topic1
  #   container_name: datagen-dc1-topic1
  #   depends_on:
  #     - broker-dc1
  #     - schema-registry-dc1
  #   volumes:
  #     - $PWD/schema-dc1.avro:/tmp/schema-dc1.avro
  #     - $PWD/datagen-properties/datagen-dc1-topic1.properties:/tmp/datagen.properties
  #   command: "bash -c 'echo Waiting for Kafka to be ready... && \
  #                      cub kafka-ready -b broker-dc1:29091 1 90 && \
  #                      echo Waiting for Confluent Schema Registry to be ready... && \
  #                      sleep 50 && \
  #                      cub sr-ready schema-registry-dc1 8081 90 && \
  #                      sleep 10 && \
  #                      /usr/bin/ksql-datagen schema=/tmp/schema-dc1.avro key=userid format=avro topic=topic1 msgRate=1 schemaRegistryUrl=http://schema-registry-dc1:8081 bootstrap-server=broker-dc1:29091 propertiesFile=/tmp/datagen.properties'"
  #   environment:
  #     KSQL_CONFIG_DIR: "/etc/ksql"
  #     KSQL_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/ksql/log4j-silent.properties"
  #     STREAMS_BOOTSTRAP_SERVERS: broker-dc1:29091
  #     STREAMS_SCHEMA_REGISTRY_HOST: schema-registry-dc1
  #     STREAMS_SCHEMA_REGISTRY_PORT: 8081

  # datagen-dc1-topic2:
  #   image: ${REPOSITORY}/ksqldb-examples:${CONFLUENT_DOCKER_TAG}
  #   hostname: datagen-dc1-topic2
  #   container_name: datagen-dc1-topic2
  #   depends_on:
  #     - broker-dc1
  #     - schema-registry-dc1
  #   volumes:
  #     - $PWD/datagen-properties/datagen-dc1-topic2.properties:/tmp/datagen.properties
  #   command: "bash -c 'echo Waiting for Kafka to be ready... && \
  #                      cub kafka-ready -b broker-dc1:29091 1 90 && \
  #                      echo Waiting for Confluent Schema Registry to be ready... && \
  #                      sleep 50 && \
  #                      cub sr-ready schema-registry-dc1 8081 90 && \
  #                      sleep 20 && \
  #                      /usr/bin/ksql-datagen format=avro quickstart=users topic=topic2 msgRate=1 schemaRegistryUrl=http://schema-registry-dc1:8081 bootstrap-server=broker-dc1:29091 propertiesFile=/tmp/datagen.properties'"
  #   environment:
  #     KSQL_CONFIG_DIR: "/etc/ksql"
  #     KSQL_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/ksql/log4j-silent.properties"
  #     STREAMS_BOOTSTRAP_SERVERS: broker-dc1:29091
  #     STREAMS_SCHEMA_REGISTRY_HOST: schema-registry-dc1
  #     STREAMS_SCHEMA_REGISTRY_PORT: 8081

  # datagen-dc2-topic1:
  #   image: ${REPOSITORY}/ksqldb-examples:${CONFLUENT_DOCKER_TAG}
  #   hostname: datagen-dc2-topic1
  #   container_name: datagen-dc2-topic1
  #   depends_on:
  #     - broker-dc2
  #     - schema-registry-dc2
  #   volumes:
  #     - $PWD/schema-dc2.avro:/tmp/schema-dc2.avro
  #     - $PWD/datagen-properties/datagen-dc2.properties:/tmp/datagen.properties
  #   command: "bash -c 'echo Waiting for Kafka to be ready... && \
  #                      cub kafka-ready -b broker-dc2:29092 1 90 && \
  #                      echo Waiting for Confluent Schema Registry to be ready... && \
  #                      sleep 50 && \
  #                      cub sr-ready schema-registry-dc2 8082 90 && \
  #                      sleep 10 && \
  #                      /usr/bin/ksql-datagen schema=/tmp/schema-dc2.avro key=userid format=avro topic=topic1 msgRate=1 schemaRegistryUrl=http://schema-registry-dc2:8082 bootstrap-server=broker-dc2:29092 propertiesFile=/tmp/datagen.properties'"
  #   environment:
  #     KSQL_CONFIG_DIR: "/etc/ksql"
  #     KSQL_LOG4J_OPTS: "-Dlog4j.configuration=file:/etc/ksql/log4j-silent.properties"
  #     STREAMS_BOOTSTRAP_SERVERS: broker-dc2:29092
  #     STREAMS_SCHEMA_REGISTRY_HOST: schema-registry-dc2
  #     STREAMS_SCHEMA_REGISTRY_PORT: 8082
